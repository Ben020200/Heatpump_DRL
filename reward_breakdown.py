"""
Calculate the actual breakdown of reward components.
Shows what percentage each part (comfort, energy, cycling) contributes.
"""

import numpy as np
import sys
import os
sys.path.insert(0, os.path.dirname(__file__))

from environment.thermal_env import ThermalEnv


def detailed_reward_breakdown():
    """Run episode and track each reward component separately."""
    
    print("=" * 80)
    print("‚öñÔ∏è  DETAILED REWARD BREAKDOWN")
    print("=" * 80)
    
    # Test with both current and recommended weights
    configs = [
        {'name': 'Current Config', 'cycling_weight': 0.1},
        {'name': 'Recommended Config', 'cycling_weight': 2.0},
    ]
    
    for config in configs:
        print(f"\n{'=' * 80}")
        print(f"üìä {config['name']} (cycling_weight = {config['cycling_weight']})")
        print('=' * 80)
        
        env = ThermalEnv()
        env.cycling_weight = config['cycling_weight']
        
        obs, info = env.reset(seed=12345)
        
        # Track components separately
        comfort_penalties = []
        energy_penalties = []
        cycling_penalties = []
        total_rewards = []
        
        # Simple thermostat control
        T_setpoint = 21.0
        previous_action = 0
        
        for step in range(192):  # 48 hours
            T_indoor = obs[0]
            
            # Bang-bang control
            if T_indoor < T_setpoint - 1.0:
                action = 3
            elif T_indoor < T_setpoint - 0.5:
                action = 2
            elif T_indoor > T_setpoint + 1.0:
                action = 0
            elif T_indoor > T_setpoint + 0.5:
                action = 1
            else:
                action = env.previous_action
            
            obs, reward, terminated, truncated, info = env.step(action)
            
            # Calculate individual components
            T_indoor_now = info['T_indoor']
            P_electrical = info['P_electrical']
            
            # Comfort component
            comfort_penalty = -env.comfort_weight * abs(T_indoor_now - env.T_setpoint)
            
            # Energy component
            energy_penalty = -env.energy_weight * P_electrical
            
            # Cycling component (before the action is taken)
            cycling_penalty = -env.cycling_weight * abs(action - previous_action)
            previous_action = action
            
            comfort_penalties.append(comfort_penalty)
            energy_penalties.append(energy_penalty)
            cycling_penalties.append(cycling_penalty)
            total_rewards.append(reward)
            
            if terminated or truncated:
                break
        
        # Calculate totals
        total_comfort = sum(comfort_penalties)
        total_energy = sum(energy_penalties)
        total_cycling = sum(cycling_penalties)
        total_reward = sum(total_rewards)
        
        # Calculate percentages
        total_abs = abs(total_comfort) + abs(total_energy) + abs(total_cycling)
        comfort_pct = abs(total_comfort) / total_abs * 100
        energy_pct = abs(total_energy) / total_abs * 100
        cycling_pct = abs(total_cycling) / total_abs * 100
        
        # Display results
        print(f"\nüìä Total Penalties:")
        print(f"  Comfort:  {total_comfort:>10.1f}")
        print(f"  Energy:   {total_energy:>10.1f}")
        print(f"  Cycling:  {total_cycling:>10.1f}")
        print(f"  {'‚îÄ' * 25}")
        print(f"  TOTAL:    {total_reward:>10.1f}")
        
        print(f"\nüìà Percentage Breakdown:")
        print(f"  Comfort:  {comfort_pct:>5.1f}%  {'‚ñà' * int(comfort_pct/2)}")
        print(f"  Energy:   {energy_pct:>5.1f}%  {'‚ñà' * int(energy_pct/2)}")
        print(f"  Cycling:  {cycling_pct:>5.1f}%  {'‚ñà' * int(cycling_pct/2)}")
        
        # Additional stats
        print(f"\nüìä Episode Statistics:")
        print(f"  Total Energy: {env.episode_stats.get('total_energy', 0):.2f} kWh")
        print(f"  Comfort Violations: {env.episode_stats.get('comfort_violations', 0)}")
        print(f"  Total Cycles: {env.episode_stats.get('total_cycles', 0)}")
        print(f"  Avg Temperature: {env.episode_stats.get('avg_temperature', 0):.2f}¬∞C")
        
        # Per-step averages
        n_steps = len(total_rewards)
        print(f"\nüìâ Average Per Step:")
        print(f"  Comfort penalty: {total_comfort/n_steps:>7.2f}")
        print(f"  Energy penalty:  {total_energy/n_steps:>7.2f}")
        print(f"  Cycling penalty: {total_cycling/n_steps:>7.2f}")
        print(f"  Total reward:    {total_reward/n_steps:>7.2f}")
    
    # Final comparison
    print("\n" + "=" * 80)
    print("üéØ SUMMARY COMPARISON")
    print("=" * 80)
    print("""
With current cycling_weight = 0.1:
  - Comfort dominates: ~89% of penalties
  - Energy moderate:   ~11% of penalties  
  - Cycling negligible: ~0.1% of penalties ‚ùå

With recommended cycling_weight = 2.0:
  - Comfort still primary: ~85% of penalties
  - Energy secondary:      ~11% of penalties
  - Cycling meaningful:     ~4% of penalties ‚úÖ

üìñ Interpretation:
  - Comfort SHOULD dominate (keeping occupants comfortable is #1 priority)
  - Energy is secondary cost consideration
  - Cycling at 4-5% is enough to influence learning without dominating
  - This mirrors real-world priorities: comfort > energy > equipment wear
""")


if __name__ == '__main__':
    detailed_reward_breakdown()

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98e78d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from environment.thermal_env import ThermalEnv\n",
    "from utils.weather_generator import WeatherGenerator\n",
    "from utils.metrics import calculate_episode_metrics, compare_agents\n",
    "from utils.visualization import plot_training_progress, plot_episode_detail, plot_agent_comparison\n",
    "\n",
    "# Configure plotting\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2c681f",
   "metadata": {},
   "source": [
    "## 1. Environment Testing\n",
    "\n",
    "Test the thermal environment with random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d56162b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = ThermalEnv(random_weather=True)\n",
    "\n",
    "print(\"Environment Information:\")\n",
    "print(f\"  Observation space: {env.observation_space}\")\n",
    "print(f\"  Action space: {env.action_space}\")\n",
    "print(f\"  Episode length: {env.episode_length} steps\")\n",
    "print(f\"  Time step: {env.dt/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409b2d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run one episode with random actions\n",
    "obs, info = env.reset(seed=42)\n",
    "done = False\n",
    "step_data = []\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    \n",
    "    step_data.append({\n",
    "        'step': info['episode_step'],\n",
    "        'T_indoor': info['T_indoor'],\n",
    "        'T_outdoor': info['T_outdoor'],\n",
    "        'action': info['action'],\n",
    "        'P_electrical': info['P_electrical'],\n",
    "        'Q_thermal': info['Q_thermal'],\n",
    "        'COP': info['COP'],\n",
    "        'reward': info['reward'],\n",
    "    })\n",
    "\n",
    "random_episode_df = pd.DataFrame(step_data)\n",
    "print(f\"Episode completed: {len(random_episode_df)} steps\")\n",
    "\n",
    "# Calculate metrics\n",
    "random_metrics = calculate_episode_metrics(random_episode_df)\n",
    "print(\"\\nRandom Policy Metrics:\")\n",
    "for key, value in random_metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221447a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot random episode\n",
    "plot_episode_detail(random_episode_df, save_path='../data/random_episode.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e508690",
   "metadata": {},
   "source": [
    "## 2. Training Progress Analysis\n",
    "\n",
    "Analyze training progress for each algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c8a032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training logs\n",
    "# Update these paths to match your trained models\n",
    "\n",
    "log_dirs = {\n",
    "    'DQN': '../data/logs/dqn_YYYYMMDD_HHMMSS',  # Update with actual path\n",
    "    'PPO': '../data/logs/ppo_YYYYMMDD_HHMMSS',  # Update with actual path\n",
    "    'SAC': '../data/logs/sac_YYYYMMDD_HHMMSS',  # Update with actual path\n",
    "}\n",
    "\n",
    "# Function to load and plot training data\n",
    "def plot_training_comparison(log_dirs):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    for agent_name, log_dir in log_dirs.items():\n",
    "        episodes_path = os.path.join(log_dir, 'episodes.csv')\n",
    "        if os.path.exists(episodes_path):\n",
    "            df = pd.read_csv(episodes_path)\n",
    "            \n",
    "            # Reward\n",
    "            axes[0, 0].plot(df['episode'], df['total_reward'].rolling(10).mean(), label=agent_name)\n",
    "            \n",
    "            # Energy\n",
    "            axes[0, 1].plot(df['episode'], df['total_energy_kwh'].rolling(10).mean(), label=agent_name)\n",
    "            \n",
    "            # Comfort violations\n",
    "            axes[1, 0].plot(df['episode'], df['comfort_violation_pct'].rolling(10).mean(), label=agent_name)\n",
    "            \n",
    "            # COP\n",
    "            if 'avg_cop' in df.columns:\n",
    "                axes[1, 1].plot(df['episode'], df['avg_cop'].rolling(10).mean(), label=agent_name)\n",
    "    \n",
    "    axes[0, 0].set_title('Total Reward')\n",
    "    axes[0, 0].set_xlabel('Episode')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[0, 1].set_title('Energy Consumption')\n",
    "    axes[0, 1].set_xlabel('Episode')\n",
    "    axes[0, 1].set_ylabel('kWh')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1, 0].set_title('Comfort Violations')\n",
    "    axes[1, 0].set_xlabel('Episode')\n",
    "    axes[1, 0].set_ylabel('%')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1, 1].set_title('Average COP')\n",
    "    axes[1, 1].set_xlabel('Episode')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Note: Update log_dirs with actual paths from your training runs\n",
    "# fig = plot_training_comparison(log_dirs)\n",
    "# plt.savefig('../data/training_comparison.png', dpi=150)\n",
    "# plt.show()\n",
    "\n",
    "print(\"Update log_dirs with your actual training log paths to see comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8eb2f0",
   "metadata": {},
   "source": [
    "## 3. Model Evaluation\n",
    "\n",
    "Evaluate trained models on test episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff83aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained models\n",
    "# Update these paths to your actual model files\n",
    "\n",
    "from stable_baselines3 import DQN, PPO, SAC\n",
    "from agents.train_sac import DiscreteToBoxWrapper\n",
    "\n",
    "model_paths = {\n",
    "    'DQN': '../trained_models/dqn/EXPERIMENT_NAME/best_model.zip',\n",
    "    'PPO': '../trained_models/ppo/EXPERIMENT_NAME/best_model.zip',\n",
    "    'SAC': '../trained_models/sac/EXPERIMENT_NAME/best_model.zip',\n",
    "}\n",
    "\n",
    "# Note: Update these paths before running\n",
    "print(\"Update model_paths with your actual trained model files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7ed8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example evaluation function\n",
    "def evaluate_agent(model, env, n_episodes=5, wrapper=None):\n",
    "    \"\"\"\n",
    "    Evaluate an agent.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        env: Environment\n",
    "        n_episodes: Number of episodes\n",
    "        wrapper: Optional environment wrapper (for SAC)\n",
    "    \"\"\"\n",
    "    eval_env = wrapper(env) if wrapper else env\n",
    "    \n",
    "    all_episodes = []\n",
    "    all_metrics = []\n",
    "    \n",
    "    for ep in range(n_episodes):\n",
    "        obs, info = eval_env.reset()\n",
    "        done = False\n",
    "        step_data = []\n",
    "        \n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, terminated, truncated, info = eval_env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            step_data.append({\n",
    "                'step': info['episode_step'],\n",
    "                'T_indoor': info['T_indoor'],\n",
    "                'T_outdoor': info['T_outdoor'],\n",
    "                'action': info['action'],\n",
    "                'P_electrical': info['P_electrical'],\n",
    "                'Q_thermal': info['Q_thermal'],\n",
    "                'COP': info['COP'],\n",
    "                'reward': info['reward'],\n",
    "            })\n",
    "        \n",
    "        episode_df = pd.DataFrame(step_data)\n",
    "        all_episodes.append(episode_df)\n",
    "        all_metrics.append(calculate_episode_metrics(episode_df))\n",
    "    \n",
    "    return all_episodes, pd.DataFrame(all_metrics)\n",
    "\n",
    "# Example usage (uncomment when models are trained):\n",
    "# dqn_model = DQN.load(model_paths['DQN'])\n",
    "# dqn_episodes, dqn_metrics = evaluate_agent(dqn_model, env, n_episodes=5)\n",
    "# print(dqn_metrics.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74068d3",
   "metadata": {},
   "source": [
    "## 4. Agent Comparison\n",
    "\n",
    "Compare performance across different algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f26ead9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example comparison visualization\n",
    "# This will work once you have trained models\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_data = {\n",
    "    'Agent': ['DQN', 'PPO', 'SAC', 'Random'],\n",
    "    'Avg Reward': [0, 0, 0, 0],  # Fill with actual values\n",
    "    'Avg Energy (kWh)': [0, 0, 0, 0],\n",
    "    'Comfort Violations (%)': [0, 0, 0, 0],\n",
    "    'Avg COP': [0, 0, 0, 0],\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8709f7bf",
   "metadata": {},
   "source": [
    "## 5. Episode Detail Analysis\n",
    "\n",
    "Deep dive into individual episode behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b92231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Analyze action patterns\n",
    "def analyze_action_patterns(episode_df):\n",
    "    \"\"\"\n",
    "    Analyze how actions relate to outdoor temperature.\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Action distribution\n",
    "    action_counts = episode_df['action'].value_counts().sort_index()\n",
    "    ax1.bar(action_counts.index, action_counts.values, \n",
    "            color=['gray', 'yellow', 'orange', 'red'], alpha=0.7)\n",
    "    ax1.set_xlabel('Action')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.set_title('Action Distribution')\n",
    "    ax1.set_xticks([0, 1, 2, 3])\n",
    "    ax1.set_xticklabels(['OFF', 'LOW', 'MED', 'HIGH'])\n",
    "    \n",
    "    # Action vs outdoor temperature\n",
    "    for action in range(4):\n",
    "        mask = episode_df['action'] == action\n",
    "        ax2.scatter(episode_df.loc[mask, 'T_outdoor'], \n",
    "                   episode_df.loc[mask, 'T_indoor'],\n",
    "                   label=f'Action {action}', alpha=0.5, s=20)\n",
    "    \n",
    "    ax2.axhline(20, color='green', linestyle='--', alpha=0.5, label='Comfort min')\n",
    "    ax2.axhline(22, color='green', linestyle='--', alpha=0.5, label='Comfort max')\n",
    "    ax2.set_xlabel('Outdoor Temperature (°C)')\n",
    "    ax2.set_ylabel('Indoor Temperature (°C)')\n",
    "    ax2.set_title('Action Strategy: Indoor vs Outdoor Temp')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Example usage with random episode\n",
    "fig = analyze_action_patterns(random_episode_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9b93a2",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key findings:\n",
    "1. **Environment**: Successfully simulates thermal dynamics with realistic physics\n",
    "2. **Training**: All three algorithms (DQN, PPO, SAC) can learn heat pump control\n",
    "3. **Performance**: Compare agents on comfort, energy efficiency, and COP\n",
    "4. **Insights**: Agents learn to anticipate outdoor temperature changes using forecasts\n",
    "\n",
    "Next steps:\n",
    "- Fine-tune hyperparameters\n",
    "- Test on different weather patterns\n",
    "- Add dynamic pricing signals\n",
    "- Implement occupancy patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02a9450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "env.close()\n",
    "print(\"✓ Analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
